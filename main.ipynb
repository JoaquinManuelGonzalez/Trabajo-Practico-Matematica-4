{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03369d0-a4dd-480c-b2e1-5702915ef549",
   "metadata": {},
   "source": [
    "# Entrega Análisis y Regresión Lineal - 2025\n",
    "\n",
    "## Integrantes:\n",
    "\n",
    "- Defelipe, Bianca Eugenia - Legajo: 21341/7.\n",
    "- Gonzalez, Joaquín Manuel - Legajo: 21247/1.\n",
    "- Loyola, Yanella Nicole - Legajo: 20912/8.\n",
    "\n",
    "## Dataset Elegido\n",
    "\n",
    "El Dataset *Obesity Levels Based On Eating Habits and Physical Activites* fue elegido por contener los suficientes atributos **númericos continuos** para el adecuado **análisis estadístico y matemático** que es requerido para satisfacer los objetivos de la **entrega asignada**.\n",
    "\n",
    "Link: [https://www.kaggle.com/datasets/fatemehmehrparvar/obesity-levels](https://www.kaggle.com/datasets/fatemehmehrparvar/obesity-levels)\n",
    "\n",
    "### Sobre los atributos del Dataset\n",
    "\n",
    "El dataset presenta una combinación de atributos donde se presentan **variables continuas** que permiten realizar un análisis de los factores asociados al nivel de obesidad. Entre esas variables destacan la **edad, altura, peso, cantidad de agua consumida y frecuencia de actividad física** de cada persona, las cuales son adecuadas para la aplicación de métodos de regresión lineal.\n",
    "\n",
    "### Motivación Personal \n",
    "\n",
    "La **motivación principal** que nos hizo elegir este dataset como grupo radica en que la obesidad es actualmente uno de los principales problemas de salud pública a nivel mundial, con consecuencias tanto físicas como psicológicas.\n",
    "\n",
    "## Regresión Lineal Simple\n",
    "\n",
    "### Definir la variable respuesta y las variables predictoras, justificando el motivo de la elección de estas\n",
    "\n",
    "**Variable Respuesta**\n",
    "- *Weight (Peso):* Variable continua que refleja directamente el estado corporal del individuo y constituye uno de los principales indicadores utilizados para determinar niveles de sobrepeso u obesidad.\n",
    "\n",
    "**Variables Predictoras**\n",
    "- *Height (Altura):* Variable continua que tiene una relación fisiológica con el peso ya que el tamaño corporal condiciona la masa total del individuo. En términos estadísticos, se espera una relación lineal positiva: a mayor altura, mayor peso promedio.\n",
    "- *Age (Edad):* Variable continua que refleja como el metabolismo y la composición corporal varían con la edad. En general, el peso tiende a aumentar con los años debido a cambios hormonales y reducción de la actividad física, lo que la convierte en una variable predictora de interés.\n",
    "- *CH2O (Consumo de Agua Diaria):* Variable continua que refleja hábitos saludables y de hidratación. Una ingesta adecuada de agua se asocia a una mejor regulación metabólica y puede influir indirectamente en el control del peso corporal.\n",
    "- *FAF (Frecuencia de Actividad Física):* Variable continua que representa el nivel de actividad física del individuo. La práctica regular de ejercicio contribuye a un mayor consumo calórico y a la regulación del metabolismo lo que suele asociarse con un mejor control del peso corporal.\n",
    "- *TUE (Tiempo de Uso de Dispositivos Tecnológicos):* Variable entera que actúa como un indicador de sedentarismo. El uso prolongado de dispositivos tecnológicos se vincula con menor gasto energético y mayor probabilidad de aumento de peso.\n",
    "- *NCP (Número de Comidas Diarias):* Variable continua que representa la cantidad de ingestas completas que la persona realiza en un día. Mantener una frecuencia adecuada de comidas contribuye a una mejor distribución calórica y puede influir en el mantenimiento del peso corporal.\n",
    "\n",
    "### Realizar un análisis de regresión lineal simple entre la variable respuesta y cada variable predictora, para completar el siguiente cuadro: \n",
    "\n",
    "**Justificación de por qué usar los valores máximos de cada Variable Predictora para el valor de x estrella:**\n",
    "- Tomar el **valor máximo** por cada variable nos permite evaluar el comportamiento del modelo en los valores extremos donde la **incertidumbre** es mayor. Además, en el contexto del análisis sobre Obesidad, creemos que interesa predecir para **individuos con características extremas** (ej: máxima altura, máxima edad). Mantenemos este criterio (valor máximo) para todas las variables ya que nos facilita la **comparación entre los intervalos de predicción** de diferentes predictores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f03cc69-bd85-47da-9e52-b40856746bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sympy import symbols, sqrt\n",
    "\n",
    "# Cargamos el Dataset\n",
    "df = pd.read_csv(\"./Dataset/ObesityDataSet_raw_and_data_sinthetic.csv\", delimiter=\",\")\n",
    "\n",
    "# Asignamos variables\n",
    "y = df[\"Weight\"] # Variable Respuesta\n",
    "var_predictoras = df[[\"Height\",\"Age\", \"CH2O\", \"FAF\", \"TUE\", \"NCP\"]] # Variables Predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eda5408-9d8d-439e-83a2-7827ddd5412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables Globales\n",
    "y_promedio = y.mean() # y barra\n",
    "n = len(y) # tamaño de la muestra\n",
    "Syy = ((y - y_promedio) ** 2).sum() # cálculo de Syy\n",
    "alpha = 0.05\n",
    "t_punto_critico = t.ppf((1 - alpha)/2, n - 2)\n",
    "\n",
    "def calcular_regresion_lineal_simple_e_intervalos(X, var_x, x_estrella):\n",
    "    \n",
    "    # Cálculo de Estadísticos de Regresión\n",
    "    x_promedio = X.mean() # x barra\n",
    "    Sxx = ((X - x_promedio) ** 2).sum() # cálculo de Sxx\n",
    "    Sxy = ((X - x_promedio) * (y - y_promedio)).sum() # cálculo de Sxy\n",
    "    beta_uno_somb = Sxy / Sxx # cálculo de beta uno sombrero\n",
    "    beta_cero_somb = y_promedio - beta_uno_somb * x_promedio # cálculo de beta cero sombrero\n",
    "    SSr = Syy - beta_uno_somb * Sxy # cálculo de SSr\n",
    "    varianza_est = SSr / (n - 2) # cálculo de la Varianza Estimada\n",
    "\n",
    "    # Definición de Recta de Regresión Estimada\n",
    "    recta_regresion_est = beta_uno_somb.round(4) * var_x + beta_cero_somb.round(4)\n",
    "\n",
    "    # Grado de Ajuste\n",
    "    R2 = 1 - (SSr / Syy) # cálculo de R2\n",
    "    r = sqrt(R2) # cálculo del Coeficiente de Correlación Lienal\n",
    "\n",
    "    # Intervalos de Confianza\n",
    "\n",
    "    # Intervalo de Confianza para B0\n",
    "    aux = t_punto_critico * sqrt(varianza_est * (1 / n + (x_promedio ** 2 / Sxx)))\n",
    "    IC_beta0_inf = beta_cero_somb - aux\n",
    "    IC_beta0_sup = beta_cero_somb + aux\n",
    "\n",
    "    # Intervalo de Confianza para B1\n",
    "    aux_2 = t_punto_critico * sqrt(varianza_est / Sxx)\n",
    "    IC_beta1_inf = beta_uno_somb - aux_2\n",
    "    IC_beta1_sup = beta_uno_somb + aux_2\n",
    "\n",
    "    # Intervalo de Confianza para la Respuesta Media\n",
    "    aux_3 = t_punto_critico * sqrt(varianza_est * (1 / n + ((x_estrella - x_promedio) ** 2 / Sxx)))\n",
    "    ICM_y_inf = beta_cero_somb + beta_uno_somb * x_estrella - aux_3\n",
    "    ICM_y_sup = beta_cero_somb + beta_uno_somb * x_estrella + aux_3\n",
    "\n",
    "    # Intervalo de Predicción\n",
    "    y_estrella = recta_regresion_est.evalf(subs = {var_x: x_estrella})\n",
    "    aux_4 = t_punto_critico * sqrt(varianza_est * (1 + 1 / n + ((x_estrella - x_promedio) ** 2) / Sxx))\n",
    "    IP_y_inf = y_estrella - aux_4\n",
    "    IP_y_sup = y_estrella + aux_4\n",
    "\n",
    "    print(f\"Recta de Regresión Estimada: {recta_regresion_est}\")\n",
    "    print(f\"varianza_est: {varianza_est:.4f}\")\n",
    "    print(f\"R2: {R2:.4f}\")\n",
    "    print(f\"Coeficiente de Correlación Lineal: {r:.4f}\")\n",
    "    print(f\"Intervalo de Confianza para B1: ({IC_beta1_inf:.4f}, {IC_beta1_sup:.4f})\")\n",
    "    print(f\"Intervalo de Confianza para B0: ({IC_beta0_inf:.4f}, {IC_beta0_sup:.4f})\")\n",
    "    print(f\"Intervalo de Confianza para la Respuesta Media, con x = {x_estrella}: ({ICM_y_inf:.4f}, {ICM_y_sup:.4f})\")\n",
    "    print(f\"Intervalo de Predicción, con x = {x_estrella}: ({IP_y_inf:.4f}, {IP_y_sup:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d39181-9477-4dac-86b6-5b65dc948358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANÁLISIS 1/6 - VARIABLE PREDICTORA: Height\n",
      "Recta de Regresión Estimada: 130.0048*x_1 - 134.6402\n",
      "varianza_est: 539.0942\n",
      "R2: 0.2145\n",
      "Coeficiente de Correlación Lineal: 0.4631\n",
      "Intervalo de Confianza para B1: (130.3446, 129.6651)\n",
      "Intervalo de Confianza para B0: (-134.0612, -135.2192)\n",
      "Intervalo de Confianza para la Respuesta Media, con x = 1.98: (122.8691, 122.6696)\n",
      "Intervalo de Predicción, con x = 1.98: (124.2288, 121.3098)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANÁLISIS 2/6 - VARIABLE PREDICTORA: Age\n",
      "Recta de Regresión Estimada: 0.836*x_2 + 66.2605\n",
      "varianza_est: 658.1433\n",
      "R2: 0.0410\n",
      "Coeficiente de Correlación Lineal: 0.2026\n",
      "Intervalo de Confianza para B1: (0.8415, 0.8305)\n",
      "Intervalo de Confianza para B0: (66.3992, 66.1218)\n",
      "Intervalo de Confianza para la Respuesta Media, con x = 61.0: (117.4625, 117.0516)\n",
      "Intervalo de Predicción, con x = 61.0: (118.8785, 115.6345)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANÁLISIS 3/6 - VARIABLE PREDICTORA: CH2O\n",
      "Recta de Regresión Estimada: 8.5705*x_3 + 69.3764\n",
      "varianza_est: 658.6924\n",
      "R2: 0.0402\n",
      "Coeficiente de Correlación Lineal: 0.2006\n",
      "Intervalo de Confianza para B1: (8.6276, 8.5133)\n",
      "Intervalo de Confianza para B0: (69.4965, 69.2564)\n",
      "Intervalo de Confianza para la Respuesta Media, con x = 3.0: (95.1545, 95.0212)\n",
      "Intervalo de Predicción, con x = 3.0: (96.6988, 93.4770)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANÁLISIS 4/6 - VARIABLE PREDICTORA: FAF\n",
      "Recta de Regresión Estimada: 88.1862 - 1.5838*x_4\n",
      "varianza_est: 684.4870\n",
      "R2: 0.0026\n",
      "Coeficiente de Correlación Lineal: 0.0514\n",
      "Intervalo de Confianza para B1: (-1.5418, -1.6258)\n",
      "Intervalo de Confianza para B0: (88.2416, 88.1307)\n",
      "Intervalo de Confianza para la Respuesta Media, con x = 3.0: (83.5256, 83.3439)\n",
      "Intervalo de Predicción, con x = 3.0: (85.0781, 81.7915)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANÁLISIS 5/6 - VARIABLE PREDICTORA: TUE\n",
      "Recta de Regresión Estimada: 88.611 - 3.078*x_5\n",
      "varianza_est: 682.7882\n",
      "R2: 0.0051\n",
      "Coeficiente de Correlación Lineal: 0.0716\n",
      "Intervalo de Confianza para B1: (-3.0194, -3.1366)\n",
      "Intervalo de Confianza para B0: (88.6635, 88.5585)\n",
      "Intervalo de Confianza para la Respuesta Media, con x = 2.0: (82.5413, 82.3686)\n",
      "Intervalo de Predicción, con x = 2.0: (84.0960, 80.8140)\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANÁLISIS 6/6 - VARIABLE PREDICTORA: NCP\n",
      "Recta de Regresión Estimada: 3.6177*x_6 + 76.8702\n",
      "varianza_est: 678.3762\n",
      "R2: 0.0115\n",
      "Coeficiente de Correlación Lineal: 0.1075\n",
      "Intervalo de Confianza para B1: (3.6634, 3.5720)\n",
      "Intervalo de Confianza para B0: (76.9980, 76.7424)\n",
      "Intervalo de Confianza para la Respuesta Media, con x = 4.0: (91.4109, 91.2713)\n",
      "Intervalo de Predicción, con x = 4.0: (92.9759, 89.7061)\n"
     ]
    }
   ],
   "source": [
    "# Diccionario para mapear nombres de variables a símbolos de SymPy\n",
    "simbolos_var = {\n",
    "    'Height': symbols('x_1', real=True),\n",
    "    'Age': symbols('x_2', real=True),\n",
    "    'CH2O': symbols('x_3', real=True),\n",
    "    'FAF': symbols('x_4', real=True),\n",
    "    'TUE': symbols('x_5', real=True),\n",
    "    'NCP': symbols('x_6', real=True)\n",
    "}\n",
    "\n",
    "# Iterar sobre cada variable predictora\n",
    "for i, columna in enumerate(var_predictoras.columns, 1):\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"ANÁLISIS {i}/6 - VARIABLE PREDICTORA: {columna}\")\n",
    "    \n",
    "    X_actual = var_predictoras[columna] # variable X actual\n",
    "    simbolo_actual = simbolos_var[columna] # símbolo correspondiente para esta variable\n",
    "    x_estrella_actual = X_actual.max()  # Usando el valor máximo de cada variable\n",
    "    \n",
    "    # Llamar a la función con los parámetros específicos de esta variable\n",
    "    calcular_regresion_lineal_simple_e_intervalos(\n",
    "        X=X_actual,\n",
    "        var_x=simbolo_actual,\n",
    "        x_estrella=x_estrella_actual\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba5496-b5cc-4b5d-ba17-d08dfb1c077d",
   "metadata": {},
   "source": [
    "#### Tabla Completa\n",
    "\n",
    "| **Y** | **ŷ = β̂₁x + β̂₀** | **σ²** | **R²** | **r** | **IC(β₁)** | **IC(β₀)** | **ICM(Y)** | **IP(Y)** |\n",
    "|:------|:-------------------|:-------:|:-------:|:------:|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "| **x₁ (Height)** | 130.0048x₁ - 134.6402 | 539.0942 | 0.2145 | 0.4631 | (130.3446, 129.6651) | (-134.0612, -135.2192) | (122.8691, 122.6696) | (124.2288, 121.3098) |\n",
    "| **x₂ (Age)** | 0.836x₂ + 66.2605 | 658.1433 | 0.0410 | 0.2026 | (0.8415, 0.8305) | (66.3992, 66.1218) | (117.4625, 117.0516) | (118.8785, 115.6345) |\n",
    "| **x₃ (CH2O)** | 8.5705x₃ + 69.3764 | 658.6924 | 0.0402 | 0.2006 | (8.6276, 8.5133) | (69.4965, 69.2564) | (95.1545, 95.0212) | (96.6988, 93.4770) |\n",
    "| **x₄ (FAF)** | 88.1862 - 1.5838x₄ | 684.4870 | 0.0026 | 0.0514 | (-1.5418, -1.6258) | (88.2416, 88.1307) | (83.5256, 83.3439) | (85.0781, 81.7915) |\n",
    "| **x₅ (TUE)** | 88.611 - 3.078x₅ | 682.7882 | 0.0051 | 0.0716 | (-3.0194, -3.1366) | (88.6635, 88.5585) | (82.5413, 82.3686) | (84.0960, 80.8140) |\n",
    "| **x₆ (NCP)** | 3.6177x₆ + 76.8702 | 678.3762 | 0.0115 | 0.1075 | (3.6634, 3.5720) | (76.9980, 76.7424) | (91.4109, 91.2713) | (92.9759, 89.7061) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a43fc-b7b5-4ee0-b236-0cb332a0fc84",
   "metadata": {},
   "source": [
    "### Seleccionar la variable predictora que mejor responde a la variable respuesta y comentar los resultados obtenidos en el cuadro sobre la misma.\n",
    "\n",
    "La **variable predictora** que mejor responde a la **variable respuesta** es aquella con mayor **Coeficiente de Correlación Lineal**. Si analizamos los valores obtenidos en la tabla podemos ver que la mejor **variable predictora** es **Height** con un **Coeficiente de Correlación Lineal** de $0.4631$. Esto nos muestra una relación física coherente con el peso del individuo. Si analizamos más información sobre la variable **Height** podemos ver que:\n",
    "- Posee una *relación positiva fuerte*, esto se ve en el valor de β₁ que es de $130$, por cada metro de altura, el peso aumenta $~130\\ kg$.\n",
    "- Presenta el intervalo de confianza para β₁ *más estrecho* entre todas las variables predictoras.\n",
    "- Presenta la mejor *Bondad de Ajuste* entre todas las *variables predictoras*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488eec5a-ba16-4d87-b397-621c4b735c75",
   "metadata": {},
   "source": [
    "## Regresión Lineal Múltiple\n",
    "\n",
    "### Estimar la ecuación de regresión usando el método de descenso del gradiente.\n",
    "\n",
    "**Notas**\n",
    "- Decidimos normalizar las variables predictoras mediante la **media y desviación estándar** ya que de esta forma, el método de **Descenso de Gradiente** puede converger más rápido y de una forma estable. Si las variables presentaran escalas con rangos de valores muy diferentes entre ellas, esto podría hacer que la optimización sea **inestable**.\n",
    "- Añadimos una columna de 1s para estimar el **intercepto** $β_0$ dentro de la notación matricial $Xβ$\n",
    "- La **fórmula** utilizada para calcular el gradiente de la función de costo MSE respecto a cada $β$ es $∇ J(β) = \\frac{1}{m} \\cdot X^T(Xβ−y)$\n",
    "- Los **coeficientes resultantes** se interpretan en unidades de **desviación estándar**. Para volver a coeficientes en la escala original, hay que transformar de vuelta (o estimar los coeficientes sobre X original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5a0495-cbb5-403c-b220-9a41cad11417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergió en la época 2589\n",
      "Coeficientes estimados: [86.58605813 12.94621698  4.82634693  3.54434853 -5.05071038 -0.87279396\n",
      "  0.3556532 ]\n",
      "Recta de predicción ŷ = 12.9462*x_1 + 4.8263*x_2 + 3.5443*x_3 - 5.0507*x_4 - 0.8728*x_5 + 0.3557*x_6 + 86.5861\n"
     ]
    }
   ],
   "source": [
    "# Normalizamos las variables predictoras\n",
    "predictoras_normalizadas = (var_predictoras - var_predictoras.mean()) / var_predictoras.std()\n",
    "\n",
    "# Agregamos columna de 1s para el Intercepto\n",
    "X_b = np.c_[np.ones((predictoras_normalizadas.shape[0], 1)), predictoras_normalizadas]\n",
    "\n",
    "# Inicialización de Parámetros\n",
    "m, n = X_b.shape\n",
    "beta_grad = np.zeros(n) # inicialización de los coeficientes en 0\n",
    "alpha = 0.01 # tasa de aprendizaje\n",
    "epochs = 3000 # cantidad de iteraciones\n",
    "min_err = 1e-6 # error mínimo para cóndición de corte\n",
    "\n",
    "# Descenso de Gradiente\n",
    "for epoch in range(epochs):\n",
    "    y_pred = X_b.dot(beta_grad) # calculamos la predicción actual\n",
    "    error = y_pred - y # calculamos el vector de errores\n",
    "    grad = (1/m) * X_b.T.dot(error) # calculamos el gradiente de la función de costo MSE\n",
    "    beta_grad -= alpha * grad # actualizamos parámetros hacia la dirección del mínimo\n",
    "\n",
    "    if np.linalg.norm(grad, ord=1) < min_err: # chequeamos condición de corte por error\n",
    "        print(f\"Convergió en la época {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"Coeficientes estimados: {beta_grad}\")\n",
    "recta_prediccion = beta_grad[0].round(4) + beta_grad[1].round(4) * simbolos_var[\"Height\"] + beta_grad[2].round(4) * simbolos_var[\"Age\"] + beta_grad[3].round(4) * simbolos_var[\"CH2O\"] + beta_grad[4].round(4) * simbolos_var[\"FAF\"] + beta_grad[5].round(4) * simbolos_var[\"TUE\"] + beta_grad[6].round(4) * simbolos_var[\"NCP\"]\n",
    "print(f\"Recta de predicción ŷ = {recta_prediccion}\")\n",
    "y_predichas_grad = X_b.dot(beta_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653fa97-721b-4fe4-9704-576960022a68",
   "metadata": {},
   "source": [
    "###  Estimar la ecuación de regresión usando el método de mínimos cuadrados. Comentar los resultados obtenidos por ambos métodos.\n",
    "\n",
    "**Notas**\n",
    "- Calculamos la solución teórica utilizando el método de la **Pseudoinversa** $\\hat{\\beta} =(X^TX)^{−1}X^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cd502b-9b39-45d6-92e6-19c5279f62db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes estimados: [86.58605813 12.94621744  4.82634664  3.54434841 -5.0507107  -0.87279417\n",
      "  0.35565302]\n",
      "Recta de predicción ŷ = 12.9462*x_1 + 4.8263*x_2 + 3.5443*x_3 - 5.0507*x_4 - 0.8728*x_5 + 0.3557*x_6 + 86.5861\n"
     ]
    }
   ],
   "source": [
    "# Agregamos columna de 1s para el Intercepto\n",
    "Z_b = np.c_[np.ones(predictoras_normalizadas.shape[0]), predictoras_normalizadas]\n",
    "\n",
    "# Mínimos cuadrados\n",
    "beta_cuad = np.linalg.inv(Z_b.T.dot(Z_b)).dot(Z_b.T).dot(y) # Calculamos la solución teórica por Pseudoinversa\n",
    "print(f\"Coeficientes estimados: {beta_cuad}\")\n",
    "recta_prediccion = beta_cuad[0].round(4) + beta_cuad[1].round(4) * simbolos_var[\"Height\"] + beta_cuad[2].round(4) * simbolos_var[\"Age\"] + beta_cuad[3].round(4) * simbolos_var[\"CH2O\"] + beta_cuad[4].round(4) * simbolos_var[\"FAF\"] + beta_cuad[5].round(4) * simbolos_var[\"TUE\"] + beta_cuad[6].round(4) * simbolos_var[\"NCP\"]\n",
    "print(f\"Recta de predicción ŷ = {recta_prediccion}\")\n",
    "y_predichas_cuad = X_b.dot(beta_cuad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b70a5-9b25-45cd-970b-c726f22eac28",
   "metadata": {},
   "source": [
    "### Comparación entre los resultados obtenidos en los 2 métodos\n",
    "\n",
    "#### Método de Descenso de Gradiente\n",
    "- Este método **convergió** en la iteración número $2589$ y dió estos **Coeficientes estimados:** \\[86.58605813 12.94621698  4.82634693  3.54434853 -5.05071038 -0.87279396 0.3556532\\]. Este método es **iterativo** y, por lo tanto, **aproximado**, ya que por cada iteración busca acercarse lo más posible al **mínimo absoluto** de la función teniendo en cuenta el **criterio de corte** por error que en nuestro caso fue establecido en $1e-6$.\n",
    "\n",
    "#### Método de Mínimos Cuadrados\n",
    "- Se utilizó la solución teórica mediante el método de la **Pseudoinversa** ($\\hat{\\beta} =(X^TX)^{−1}X^T$) que nos proporciona la solución **exacta** para el dataset seleccionado. Los **Coeficientes estimados** fueron: \\[86.58605813 12.94621744  4.82634664  3.54434841 -5.0507107  -0.87279417 0.35565302\\].\n",
    "\n",
    "Cabe destacar que ambos métodos nos dan dos conjuntos de **Coeficientes estimados muy parecidos**, esto nos deja ver que la implementación fue realizada **correctamente**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ff364-2116-4d0c-ba70-b9a4bd7e9469",
   "metadata": {},
   "source": [
    "### ¿La adición de más variables predictoras mejoró la estimación en comparación con la obtenida en el inciso c)? Explique.\n",
    "\n",
    "Si comparamos los resultados obtenidos entre el análisis de una única varaible en la **regresión lineal simple**, y el análisis de múltiples variables en la **regresión lineal múltiple** podemos ver que el **Coeficiente de Correlación** obtenido en la **regresión lineal múltiple** supera al obtenido por la variable predictora que mejor se comportaba (\"Height\") en la **regresión lineal simple**, ya que $|r_a| > |r| \\wedge |r_a|\\ \\text{se acerca más a 1 que } |r|$, por lo tanto, podemos concluir que la adición de más variables predictoras mejoró la estimación en comparación con la obtenida en el inciso c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5006e1d0-8110-4373-b3df-db46859d6807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.3072\n",
      "R² ajustado: 0.3052\n",
      "ra: 0.5524\n"
     ]
    }
   ],
   "source": [
    "SSr = ((y - y_predichas_cuad)**2).sum()\n",
    "SSt = ((y - y.mean())**2).sum()\n",
    "R2 = 1 - SSr/SSt\n",
    "print(f\"R²: {R2.round(4)}\")\n",
    "k = 6 # número de variables predictoras\n",
    "R2_adj = 1 - (1 - R2)*(len(y) - 1) / (len(y) - k - 1)\n",
    "print(f\"R² ajustado: {R2_adj.round(4)}\")\n",
    "ra = sqrt(R2_adj)\n",
    "print(f\"ra: {ra.round(4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
